# å¤–éƒ¨æ•°æ®æºåŒæ­¥åŠŸèƒ½å®ç°è®¡åˆ’

## ğŸ“‹ åŠŸèƒ½æ¦‚è¿°

å®ç°ä¸€ä¸ªç‹¬ç«‹çš„æ–°åŠŸèƒ½æ¨¡å—ï¼Œç”¨äºï¼š
1. ä»å¤–éƒ¨æ•°æ®åº“è·å–å¤šç”¨æˆ·ä¿¡æ¯
2. æŒ‰ç”¨æˆ·IDåˆ†ç»„æ€»ç»“æç‚¼ä¿¡æ¯
3. å­˜å‚¨åˆ°æœ¬åœ°OpenMemoryæ•°æ®åº“
4. å®šæ—¶è‡ªåŠ¨æ‰§è¡ŒåŒæ­¥ä»»åŠ¡

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```mermaid
flowchart TD
    A[å®šæ—¶è°ƒåº¦å™¨] --> B[å¤–éƒ¨æ•°æ®è·å–å™¨]
    B --> C[æ•°æ®æºé€‚é…å™¨]
    C --> D1[MySQLé€‚é…å™¨]
    C --> D2[PostgreSQLé€‚é…å™¨]
    C --> D3[MongoDBé€‚é…å™¨]
    C --> D4[APIé€‚é…å™¨]
    B --> E[ç”¨æˆ·IDæ˜ å°„å™¨]
    E --> F[æ•°æ®æ€»ç»“å™¨]
    F --> G[LLMæœåŠ¡]
    F --> H[è®°å¿†å­˜å‚¨å™¨]
    H --> I[OpenMemoryæ•°æ®åº“]
```

## ğŸ“ æ–‡ä»¶ç»“æ„

åœ¨ `api/app/` ç›®å½•ä¸‹åˆ›å»ºæ–°æ¨¡å—ï¼Œä¸ä¿®æ”¹ç°æœ‰ä»£ç ï¼š

```
api/app/
â”œâ”€â”€ external_sync/                    # æ–°åŠŸèƒ½æ¨¡å—ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                     # å¤–éƒ¨æ•°æ®æºé…ç½®
â”‚   â”œâ”€â”€ models.py                     # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ adapters/                     # æ•°æ®æºé€‚é…å™¨
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                   # åŸºç¡€é€‚é…å™¨æ¥å£
â”‚   â”‚   â”œâ”€â”€ mysql_adapter.py
â”‚   â”‚   â”œâ”€â”€ postgres_adapter.py
â”‚   â”‚   â”œâ”€â”€ mongodb_adapter.py
â”‚   â”‚   â””â”€â”€ api_adapter.py
â”‚   â”œâ”€â”€ services/                     # ä¸šåŠ¡æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_fetcher.py           # æ•°æ®è·å–æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ user_mapper.py            # ç”¨æˆ·IDæ˜ å°„æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ summarizer.py             # æ•°æ®æ€»ç»“æœåŠ¡
â”‚   â”‚   â””â”€â”€ memory_store.py           # è®°å¿†å­˜å‚¨æœåŠ¡
â”‚   â”œâ”€â”€ scheduler.py                  # å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
â”‚   â””â”€â”€ router.py                     # APIè·¯ç”±
â”œâ”€â”€ external_sync_config.json         # é…ç½®æ–‡ä»¶
```

## ğŸ”§ æ ¸å¿ƒå®ç°

### 1. é…ç½®æ–‡ä»¶è®¾è®¡ (external_sync_config.json)

```json
{
  "enabled": true,
  "sync_interval_minutes": 60,
  "max_retries": 3,
  "batch_size": 100,
  "sources": [
    {
      "id": "source_1",
      "name": "ç”¨æˆ·æ•°æ®åº“",
      "type": "mysql",
      "enabled": true,
      "connection": {
        "host": "env:EXTERNAL_DB_HOST",
        "port": 3306,
        "database": "env:EXTERNAL_DB_NAME",
        "username": "env:EXTERNAL_DB_USER",
        "password": "env:EXTERNAL_DB_PASSWORD"
      },
      "query": {
        "sql": "SELECT user_id, content, created_at FROM memories WHERE updated_at > :last_sync_time",
        "user_id_field": "user_id",
        "content_field": "content",
        "timestamp_field": "created_at"
      },
      "user_mapping": {
        "type": "direct",
        "source_field": "user_id",
        "transform": null
      }
    }
  ],
  "summarization": {
    "enabled": true,
    "prompt_template": "è¯·ä»ä»¥ä¸‹ä¿¡æ¯ä¸­æå–å…³äºç”¨æˆ·çš„é‡è¦äº‹å®...",
    "max_tokens": 2000,
    "batch_size": 10
  }
}
```

### 2. åŸºç¡€é€‚é…å™¨æ¥å£ (adapters/base.py)

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from datetime import datetime

class BaseDataSourceAdapter(ABC):
    """å¤–éƒ¨æ•°æ®æºé€‚é…å™¨åŸºç±»"""
    
    def __init__(self, config: dict):
        self.config = config
        self.source_id = config.get("id")
        self.source_name = config.get("name")
    
    @abstractmethod
    async def connect(self) -> bool:
        """å»ºç«‹è¿æ¥"""
        pass
    
    @abstractmethod
    async def disconnect(self) -> None:
        """æ–­å¼€è¿æ¥"""
        pass
    
    @abstractmethod
    async def fetch_data(
        self, 
        last_sync_time: Optional[datetime] = None,
        batch_size: int = 100
    ) -> List[Dict[str, Any]]:
        """è·å–æ•°æ®"""
        pass
    
    @abstractmethod
    async def get_user_ids(self) -> List[str]:
        """è·å–æ‰€æœ‰ç”¨æˆ·ID"""
        pass
    
    @abstractmethod
    async def fetch_user_data(
        self, 
        user_id: str,
        last_sync_time: Optional[datetime] = None
    ) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šç”¨æˆ·çš„æ•°æ®"""
        pass
```

### 3. MySQLé€‚é…å™¨ç¤ºä¾‹ (adapters/mysql_adapter.py)

```python
import aiomysql
from typing import List, Dict, Any, Optional
from datetime import datetime
from .base import BaseDataSourceAdapter

class MySQLAdapter(BaseDataSourceAdapter):
    """MySQLæ•°æ®æºé€‚é…å™¨"""
    
    def __init__(self, config: dict):
        super().__init__(config)
        self.pool = None
        self.connection_config = config.get("connection", {})
        self.query_config = config.get("query", {})
    
    async def connect(self) -> bool:
        try:
            self.pool = await aiomysql.create_pool(
                host=self._resolve_env(self.connection_config.get("host")),
                port=self.connection_config.get("port", 3306),
                db=self._resolve_env(self.connection_config.get("database")),
                user=self._resolve_env(self.connection_config.get("username")),
                password=self._resolve_env(self.connection_config.get("password")),
                autocommit=True
            )
            return True
        except Exception as e:
            logger.error(f"MySQLè¿æ¥å¤±è´¥: {e}")
            return False
    
    async def disconnect(self) -> None:
        if self.pool:
            self.pool.close()
            await self.pool.wait_closed()
    
    async def fetch_data(
        self, 
        last_sync_time: Optional[datetime] = None,
        batch_size: int = 100
    ) -> List[Dict[str, Any]]:
        sql = self.query_config.get("sql")
        async with self.pool.acquire() as conn:
            async with conn.cursor(aiomysql.DictCursor) as cursor:
                await cursor.execute(sql, {"last_sync_time": last_sync_time})
                return await cursor.fetchmany(batch_size)
    
    async def get_user_ids(self) -> List[str]:
        user_id_field = self.query_config.get("user_id_field", "user_id")
        sql = f"SELECT DISTINCT {user_id_field} FROM ({self.query_config.get('sql')}) AS t"
        async with self.pool.acquire() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(sql.replace(":last_sync_time", "NULL"))
                rows = await cursor.fetchall()
                return [str(row[0]) for row in rows]
    
    async def fetch_user_data(
        self, 
        user_id: str,
        last_sync_time: Optional[datetime] = None
    ) -> List[Dict[str, Any]]:
        user_id_field = self.query_config.get("user_id_field", "user_id")
        base_sql = self.query_config.get("sql")
        sql = f"{base_sql} AND {user_id_field} = :user_id"
        async with self.pool.acquire() as conn:
            async with conn.cursor(aiomysql.DictCursor) as cursor:
                await cursor.execute(sql, {
                    "last_sync_time": last_sync_time,
                    "user_id": user_id
                })
                return await cursor.fetchall()
    
    def _resolve_env(self, value: str) -> str:
        """è§£æç¯å¢ƒå˜é‡"""
        if value and value.startswith("env:"):
            import os
            return os.getenv(value[4:], "")
        return value
```

### 4. ç”¨æˆ·IDæ˜ å°„æœåŠ¡ (services/user_mapper.py)

```python
from typing import Any, Optional
from uuid import UUID
import re

class UserIdMapper:
    """ç”¨æˆ·IDæ˜ å°„æœåŠ¡"""
    
    TRANSFORM_FUNCTIONS = {
        "direct": lambda x: str(x),
        "uuid_to_string": lambda x: str(x).replace("-", ""),
        "string_to_uuid": lambda x: str(UUID(x)),
        "lowercase": lambda x: str(x).lower(),
        "uppercase": lambda x: str(x).upper(),
        "prefix_remove": lambda x, prefix: str(x).replace(prefix, "", 1),
        "prefix_add": lambda x, prefix: f"{prefix}{x}",
    }
    
    def __init__(self, mapping_config: dict):
        self.config = mapping_config
        self.transform_type = mapping_config.get("type", "direct")
        self.source_field = mapping_config.get("source_field", "user_id")
        self.transform_params = mapping_config.get("transform_params", {})
    
    def map_user_id(self, source_data: dict) -> Optional[str]:
        """å°†å¤–éƒ¨ç”¨æˆ·IDæ˜ å°„åˆ°æœ¬åœ°ç”¨æˆ·ID"""
        source_value = source_data.get(self.source_field)
        if source_value is None:
            return None
        
        transform_func = self.TRANSFORM_FUNCTIONS.get(self.transform_type)
        if not transform_func:
            return str(source_value)
        
        try:
            if self.transform_params:
                return transform_func(source_value, **self.transform_params)
            return transform_func(source_value)
        except Exception as e:
            logger.error(f"ç”¨æˆ·IDæ˜ å°„å¤±è´¥: {e}")
            return str(source_value)
    
    def reverse_map(self, local_user_id: str) -> str:
        """åå‘æ˜ å°„ï¼ˆæœ¬åœ°IDåˆ°å¤–éƒ¨IDï¼‰"""
        # æ ¹æ®éœ€è¦å®ç°åå‘æ˜ å°„é€»è¾‘
        return local_user_id
```

### 5. æ•°æ®æ€»ç»“æœåŠ¡ (services/summarizer.py)

```python
from typing import List, Dict, Any
from app.utils.memory import get_memory_client
import json

class DataSummarizer:
    """æ•°æ®æ€»ç»“æœåŠ¡ - ä½¿ç”¨ç°æœ‰LLMé…ç½®"""
    
    DEFAULT_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æå–åŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹ç”¨æˆ·æ•°æ®ä¸­æå–é‡è¦çš„äº‹å®ä¿¡æ¯ã€‚

ç”¨æˆ·ID: {user_id}
æ•°æ®æ¥æº: {source_name}
åŸå§‹æ•°æ®:
{raw_data}

è¯·æå–å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
1. ç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯ï¼ˆå§“åã€å¹´é¾„ã€èŒä¸šç­‰ï¼‰
2. ç”¨æˆ·çš„åå¥½å’Œä¹ æƒ¯
3. ç”¨æˆ·çš„é‡è¦è¡Œä¸ºæˆ–äº‹ä»¶
4. å…¶ä»–å€¼å¾—è®°å¿†çš„ä¿¡æ¯

è§„åˆ™ï¼š
- åªæå–æ˜ç¡®çš„äº‹å®ä¿¡æ¯ï¼Œä¸è¦æ¨æµ‹
- æ¯æ¡ä¿¡æ¯åº”è¯¥æ˜¯ä¸€ä¸ªå®Œæ•´ã€ç‹¬ç«‹çš„é™ˆè¿°
- ä½¿ç”¨ç®€æ´ã€è‡ªç„¶çš„ä¸­æ–‡è¡¨è¾¾
- å¦‚æœæ²¡æœ‰æ˜ç¡®çš„äº‹å®ä¿¡æ¯ï¼Œè¿”å›ç©ºåˆ—è¡¨

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{"facts": ["äº‹å®1", "äº‹å®2", ...]}}
"""
    
    def __init__(self, config: dict = None):
        self.config = config or {}
        self.prompt_template = self.config.get("prompt_template", self.DEFAULT_PROMPT)
        self.max_tokens = self.config.get("max_tokens", 2000)
        self.batch_size = self.config.get("batch_size", 10)
    
    async def summarize_user_data(
        self,
        user_id: str,
        source_name: str,
        data_list: List[Dict[str, Any]]
    ) -> List[str]:
        """æ€»ç»“ç”¨æˆ·æ•°æ®ï¼Œè¿”å›äº‹å®åˆ—è¡¨"""
        if not data_list:
            return []
        
        # å‡†å¤‡æ•°æ®
        raw_data = json.dumps(data_list, ensure_ascii=False, indent=2)
        
        # æ„å»ºæç¤ºè¯
        prompt = self.prompt_template.format(
            user_id=user_id,
            source_name=source_name,
            raw_data=raw_data[:10000]  # é™åˆ¶é•¿åº¦
        )
        
        # è°ƒç”¨LLM
        try:
            memory_client = get_memory_client()
            if memory_client and hasattr(memory_client, 'llm'):
                response = await self._call_llm(memory_client.llm, prompt)
                return self._parse_response(response)
        except Exception as e:
            logger.error(f"æ•°æ®æ€»ç»“å¤±è´¥: {e}")
        
        return []
    
    async def _call_llm(self, llm, prompt: str) -> str:
        """è°ƒç”¨LLMæœåŠ¡"""
        # ä½¿ç”¨ç°æœ‰çš„LLMé…ç½®
        response = llm.generate_response(
            messages=[{"role": "user", "content": prompt}]
        )
        return response
    
    def _parse_response(self, response: str) -> List[str]:
        """è§£æLLMå“åº”"""
        try:
            # å°è¯•è§£æJSON
            data = json.loads(response)
            if isinstance(data, dict) and "facts" in data:
                return data["facts"]
            if isinstance(data, list):
                return data
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯JSONï¼ŒæŒ‰è¡Œåˆ†å‰²
            lines = response.strip().split("\n")
            return [line.strip() for line in lines if line.strip()]
        return []
```

### 6. è®°å¿†å­˜å‚¨æœåŠ¡ (services/memory_store.py)

```python
from typing import List, Optional
from uuid import uuid4
from datetime import datetime, UTC
from sqlalchemy.orm import Session

from app.database import SessionLocal
from app.models import Memory, MemoryState, MemoryStatusHistory, User, App
from app.utils.db import get_or_create_user, get_or_create_app
from app.utils.memory import get_memory_client

class MemoryStoreService:
    """è®°å¿†å­˜å‚¨æœåŠ¡ - å¤ç”¨ç°æœ‰å­˜å‚¨é€»è¾‘"""
    
    APP_NAME = "external_data_sync"
    
    def __init__(self):
        self.memory_client = None
    
    async def store_user_memories(
        self,
        user_id: str,
        facts: List[str],
        source_name: str,
        source_id: str
    ) -> int:
        """å­˜å‚¨ç”¨æˆ·è®°å¿†"""
        if not facts:
            return 0
        
        db = SessionLocal()
        stored_count = 0
        
        try:
            # è·å–æˆ–åˆ›å»ºç”¨æˆ·å’Œåº”ç”¨
            user = get_or_create_user(db, user_id)
            app = get_or_create_app(db, user, self.APP_NAME)
            
            for fact in facts:
                try:
                    # å°è¯•ä½¿ç”¨memory_clientå­˜å‚¨ï¼ˆåŒ…å«å‘é‡åŒ–ï¼‰
                    memory_id = await self._store_with_client(
                        user_id, fact, source_name, source_id
                    )
                    
                    if memory_id:
                        # åŒæ­¥åˆ°æœ¬åœ°æ•°æ®åº“
                        await self._sync_to_database(
                            db, user, app, memory_id, fact, source_name, source_id
                        )
                    else:
                        # é™çº§ï¼šç›´æ¥å­˜å‚¨åˆ°æ•°æ®åº“
                        await self._store_to_database_only(
                            db, user, app, fact, source_name, source_id
                        )
                    
                    stored_count += 1
                    
                except Exception as e:
                    logger.error(f"å­˜å‚¨è®°å¿†å¤±è´¥: {e}")
                    continue
            
            db.commit()
            
        finally:
            db.close()
        
        return stored_count
    
    async def _store_with_client(
        self,
        user_id: str,
        fact: str,
        source_name: str,
        source_id: str
    ) -> Optional[str]:
        """ä½¿ç”¨memory_clientå­˜å‚¨"""
        try:
            if not self.memory_client:
                self.memory_client = get_memory_client()
            
            if not self.memory_client:
                return None
            
            response = self.memory_client.add(
                fact,
                user_id=user_id,
                metadata={
                    "source_app": self.APP_NAME,
                    "external_source": source_name,
                    "external_source_id": source_id,
                    "sync_type": "external_data"
                }
            )
            
            if isinstance(response, dict) and "results" in response:
                for result in response["results"]:
                    if result.get("event") in ["ADD", "UPDATE"]:
                        return result.get("id")
            
            return None
            
        except Exception as e:
            logger.warning(f"Memory clientå­˜å‚¨å¤±è´¥: {e}")
            return None
    
    async def _sync_to_database(
        self,
        db: Session,
        user: User,
        app: App,
        memory_id: str,
        fact: str,
        source_name: str,
        source_id: str
    ):
        """åŒæ­¥åˆ°æœ¬åœ°æ•°æ®åº“"""
        from uuid import UUID
        
        memory_uuid = UUID(memory_id)
        existing = db.query(Memory).filter(Memory.id == memory_uuid).first()
        
        if not existing:
            memory = Memory(
                id=memory_uuid,
                user_id=user.id,
                app_id=app.id,
                content=fact,
                metadata_={
                    "external_source": source_name,
                    "external_source_id": source_id,
                    "sync_type": "external_data"
                },
                state=MemoryState.active
            )
            db.add(memory)
            
            # åˆ›å»ºå†å²è®°å½•
            history = MemoryStatusHistory(
                memory_id=memory_uuid,
                changed_by=user.id,
                old_state=MemoryState.deleted,
                new_state=MemoryState.active
            )
            db.add(history)
    
    async def _store_to_database_only(
        self,
        db: Session,
        user: User,
        app: App,
        fact: str,
        source_name: str,
        source_id: str
    ):
        """ä»…å­˜å‚¨åˆ°æ•°æ®åº“ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        memory = Memory(
            id=uuid4(),
            user_id=user.id,
            app_id=app.id,
            content=fact,
            metadata_={
                "external_source": source_name,
                "external_source_id": source_id,
                "sync_type": "external_data",
                "vector_pending": True  # æ ‡è®°å¾…å‘é‡åŒ–
            },
            state=MemoryState.active
        )
        db.add(memory)
        
        history = MemoryStatusHistory(
            memory_id=memory.id,
            changed_by=user.id,
            old_state=MemoryState.deleted,
            new_state=MemoryState.active
        )
        db.add(history)
```

### 7. å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ (scheduler.py)

```python
import logging
import os
import json
from datetime import datetime, UTC
from typing import Dict, List, Optional
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger

from app.database import SessionLocal
from app.models import User

logger = logging.getLogger(__name__)

class ExternalDataSyncScheduler:
    """å¤–éƒ¨æ•°æ®åŒæ­¥è°ƒåº¦å™¨"""
    
    def __init__(self):
        self.scheduler = BackgroundScheduler()
        self.config = self._load_config()
        self.adapters: Dict[str, BaseDataSourceAdapter] = {}
        self.user_mapper = None
        self.summarizer = None
        self.memory_store = None
        self.last_sync_times: Dict[str, datetime] = {}
    
    def _load_config(self) -> dict:
        """åŠ è½½é…ç½®"""
        config_path = os.path.join(
            os.path.dirname(__file__), 
            "..", 
            "external_sync_config.json"
        )
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            logger.warning("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {"enabled": False, "sources": []}
    
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if not self.config.get("enabled", False):
            logger.info("å¤–éƒ¨æ•°æ®åŒæ­¥åŠŸèƒ½å·²ç¦ç”¨")
            return
        
        # åˆå§‹åŒ–æœåŠ¡
        self._init_services()
        
        # æ·»åŠ å®šæ—¶ä»»åŠ¡
        interval = self.config.get("sync_interval_minutes", 60)
        self.scheduler.add_job(
            self.sync_all_sources,
            trigger=IntervalTrigger(minutes=interval),
            id="external_data_sync",
            name="å¤–éƒ¨æ•°æ®åŒæ­¥ä»»åŠ¡",
            replace_existing=True
        )
        
        self.scheduler.start()
        logger.info(f"å¤–éƒ¨æ•°æ®åŒæ­¥è°ƒåº¦å™¨å·²å¯åŠ¨ï¼Œé—´éš”: {interval}åˆ†é’Ÿ")
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        if self.scheduler.running:
            self.scheduler.shutdown()
            logger.info("å¤–éƒ¨æ•°æ®åŒæ­¥è°ƒåº¦å™¨å·²åœæ­¢")
    
    def _init_services(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        from .services.summarizer import DataSummarizer
        from .services.memory_store import MemoryStoreService
        
        self.summarizer = DataSummarizer(self.config.get("summarization", {}))
        self.memory_store = MemoryStoreService()
        
        # åˆå§‹åŒ–æ•°æ®æºé€‚é…å™¨
        for source_config in self.config.get("sources", []):
            if not source_config.get("enabled", True):
                continue
            
            adapter = self._create_adapter(source_config)
            if adapter:
                self.adapters[source_config["id"]] = adapter
    
    def _create_adapter(self, config: dict) -> Optional[BaseDataSourceAdapter]:
        """åˆ›å»ºæ•°æ®æºé€‚é…å™¨"""
        source_type = config.get("type", "").lower()
        
        adapter_map = {
            "mysql": "MySQLAdapter",
            "postgresql": "PostgreSQLAdapter",
            "postgres": "PostgreSQLAdapter",
            "mongodb": "MongoDBAdapter",
            "api": "APIAdapter",
        }
        
        adapter_class_name = adapter_map.get(source_type)
        if not adapter_class_name:
            logger.error(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {source_type}")
            return None
        
        try:
            module = __import__(
                f"app.external_sync.adapters.{source_type}_adapter",
                fromlist=[adapter_class_name]
            )
            adapter_class = getattr(module, adapter_class_name)
            return adapter_class(config)
        except Exception as e:
            logger.error(f"åˆ›å»ºé€‚é…å™¨å¤±è´¥: {e}")
            return None
    
    async def sync_all_sources(self):
        """åŒæ­¥æ‰€æœ‰æ•°æ®æº"""
        logger.info("å¼€å§‹æ‰§è¡Œå¤–éƒ¨æ•°æ®åŒæ­¥ä»»åŠ¡")
        
        for source_id, adapter in self.adapters.items():
            try:
                await self.sync_source(source_id, adapter)
            except Exception as e:
                logger.error(f"åŒæ­¥æ•°æ®æº {source_id} å¤±è´¥: {e}")
        
        logger.info("å¤–éƒ¨æ•°æ®åŒæ­¥ä»»åŠ¡å®Œæˆ")
    
    async def sync_source(self, source_id: str, adapter: BaseDataSourceAdapter):
        """åŒæ­¥å•ä¸ªæ•°æ®æº"""
        logger.info(f"å¼€å§‹åŒæ­¥æ•°æ®æº: {source_id}")
        
        # è¿æ¥æ•°æ®æº
        if not await adapter.connect():
            logger.error(f"è¿æ¥æ•°æ®æº {source_id} å¤±è´¥")
            return
        
        try:
            # è·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´
            last_sync = self.last_sync_times.get(source_id)
            
            # è·å–æ‰€æœ‰ç”¨æˆ·ID
            user_ids = await adapter.get_user_ids()
            logger.info(f"æ•°æ®æº {source_id} å…±æœ‰ {len(user_ids)} ä¸ªç”¨æˆ·")
            
            # æŒ‰ç”¨æˆ·åŒæ­¥
            for user_id in user_ids:
                await self.sync_user_from_source(
                    source_id, adapter, user_id, last_sync
                )
            
            # æ›´æ–°åŒæ­¥æ—¶é—´
            self.last_sync_times[source_id] = datetime.now(UTC)
            
        finally:
            await adapter.disconnect()
    
    async def sync_user_from_source(
        self,
        source_id: str,
        adapter: BaseDataSourceAdapter,
        user_id: str,
        last_sync: Optional[datetime]
    ):
        """åŒæ­¥å•ä¸ªç”¨æˆ·çš„æ•°æ®"""
        try:
            # è·å–ç”¨æˆ·æ•°æ®
            user_data = await adapter.fetch_user_data(user_id, last_sync)
            
            if not user_data:
                return
            
            logger.info(f"ç”¨æˆ· {user_id} ä» {source_id} è·å–åˆ° {len(user_data)} æ¡æ•°æ®")
            
            # æ˜ å°„ç”¨æˆ·ID
            source_config = next(
                (s for s in self.config.get("sources", []) if s["id"] == source_id),
                {}
            )
            mapping_config = source_config.get("user_mapping", {})
            
            from .services.user_mapper import UserIdMapper
            mapper = UserIdMapper(mapping_config)
            local_user_id = mapper.map_user_id({"user_id": user_id})
            
            if not local_user_id:
                logger.warning(f"ç”¨æˆ·IDæ˜ å°„å¤±è´¥: {user_id}")
                return
            
            # æ€»ç»“æ•°æ®
            facts = await self.summarizer.summarize_user_data(
                local_user_id,
                adapter.source_name,
                user_data
            )
            
            if not facts:
                logger.info(f"ç”¨æˆ· {local_user_id} æ²¡æœ‰æå–åˆ°æœ‰æ•ˆäº‹å®")
                return
            
            logger.info(f"ç”¨æˆ· {local_user_id} æå–åˆ° {len(facts)} æ¡äº‹å®")
            
            # å­˜å‚¨è®°å¿†
            stored = await self.memory_store.store_user_memories(
                local_user_id,
                facts,
                adapter.source_name,
                source_id
            )
            
            logger.info(f"ç”¨æˆ· {local_user_id} æˆåŠŸå­˜å‚¨ {stored} æ¡è®°å¿†")
            
        except Exception as e:
            logger.error(f"åŒæ­¥ç”¨æˆ· {user_id} æ•°æ®å¤±è´¥: {e}")
    
    async def trigger_sync_now(self, source_id: Optional[str] = None):
        """æ‰‹åŠ¨è§¦å‘åŒæ­¥"""
        if source_id:
            adapter = self.adapters.get(source_id)
            if adapter:
                await self.sync_source(source_id, adapter)
        else:
            await self.sync_all_sources()
    
    def get_status(self) -> dict:
        """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
        return {
            "enabled": self.config.get("enabled", False),
            "running": self.scheduler.running if hasattr(self, "scheduler") else False,
            "sources_count": len(self.adapters),
            "last_sync_times": {
                k: v.isoformat() for k, v in self.last_sync_times.items()
            },
            "next_run": self._get_next_run_time()
        }
    
    def _get_next_run_time(self) -> Optional[str]:
        """è·å–ä¸‹æ¬¡è¿è¡Œæ—¶é—´"""
        jobs = self.scheduler.get_jobs()
        if jobs:
            next_run = jobs[0].next_run_time
            return next_run.isoformat() if next_run else None
        return None


# å…¨å±€è°ƒåº¦å™¨å®ä¾‹
_scheduler: Optional[ExternalDataSyncScheduler] = None

def get_external_sync_scheduler() -> ExternalDataSyncScheduler:
    """è·å–è°ƒåº¦å™¨å®ä¾‹"""
    global _scheduler
    if _scheduler is None:
        _scheduler = ExternalDataSyncScheduler()
    return _scheduler

def start_external_sync_scheduler():
    """å¯åŠ¨å¤–éƒ¨æ•°æ®åŒæ­¥è°ƒåº¦å™¨"""
    scheduler = get_external_sync_scheduler()
    scheduler.start()

def stop_external_sync_scheduler():
    """åœæ­¢å¤–éƒ¨æ•°æ®åŒæ­¥è°ƒåº¦å™¨"""
    global _scheduler
    if _scheduler:
        _scheduler.stop()
```

### 8. APIè·¯ç”± (router.py)

```python
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import Optional
from pydantic import BaseModel

from app.database import get_db
from .scheduler import get_external_sync_scheduler

router = APIRouter(prefix="/api/v1/external-sync", tags=["external-sync"])


class SyncTriggerRequest(BaseModel):
    source_id: Optional[str] = None


@router.get("/status")
async def get_sync_status():
    """è·å–åŒæ­¥çŠ¶æ€"""
    scheduler = get_external_sync_scheduler()
    return scheduler.get_status()


@router.post("/trigger")
async def trigger_sync(request: SyncTriggerRequest):
    """æ‰‹åŠ¨è§¦å‘åŒæ­¥"""
    scheduler = get_external_sync_scheduler()
    await scheduler.trigger_sync_now(request.source_id)
    return {"message": "åŒæ­¥ä»»åŠ¡å·²è§¦å‘"}


@router.get("/sources")
async def list_sources():
    """åˆ—å‡ºæ‰€æœ‰æ•°æ®æº"""
    scheduler = get_external_sync_scheduler()
    return {
        "sources": [
            {
                "id": source_id,
                "name": adapter.source_name,
                "type": adapter.config.get("type")
            }
            for source_id, adapter in scheduler.adapters.items()
        ]
    }
```

### 9. é›†æˆåˆ°ä¸»åº”ç”¨ (ä¿®æ”¹ main.py)

åœ¨ `main.py` ä¸­æ·»åŠ ä»¥ä¸‹ä»£ç ï¼ˆåœ¨ç°æœ‰ä»£ç åŸºç¡€ä¸Šæ·»åŠ ï¼Œä¸ä¿®æ”¹åŸæœ‰é€»è¾‘ï¼‰ï¼š

```python
# åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ å¯¼å…¥
from app.external_sync.scheduler import start_external_sync_scheduler, stop_external_sync_scheduler
from app.external_sync.router import router as external_sync_router

# åœ¨ include_router éƒ¨åˆ†æ·»åŠ 
app.include_router(external_sync_router)

# åœ¨ startup_event ä¸­æ·»åŠ 
@app.on_event("startup")
async def startup_event():
    # ... ç°æœ‰ä»£ç  ...
    
    # å¯åŠ¨å¤–éƒ¨æ•°æ®åŒæ­¥è°ƒåº¦å™¨
    try:
        start_external_sync_scheduler()
        logger.info("âœ… å¤–éƒ¨æ•°æ®åŒæ­¥è°ƒåº¦å™¨å¯åŠ¨æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ å¤–éƒ¨æ•°æ®åŒæ­¥è°ƒåº¦å™¨å¯åŠ¨å¤±è´¥: {e}")

# åœ¨ shutdown_event ä¸­æ·»åŠ 
@app.on_event("shutdown")
async def shutdown_event():
    # ... ç°æœ‰ä»£ç  ...
    
    # åœæ­¢å¤–éƒ¨æ•°æ®åŒæ­¥è°ƒåº¦å™¨
    try:
        stop_external_sync_scheduler()
        logger.info("âœ… å¤–éƒ¨æ•°æ®åŒæ­¥è°ƒåº¦å™¨å·²åœæ­¢")
    except Exception as e:
        logger.error(f"âŒ åœæ­¢å¤–éƒ¨æ•°æ®åŒæ­¥è°ƒåº¦å™¨å¤±è´¥: {e}")
```

## ğŸ“ æ€»ç»“æç¤ºè¯æ¨¡æ¿

### é»˜è®¤æ€»ç»“æç¤ºè¯

```
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¿¡æ¯æå–åŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹ç”¨æˆ·æ•°æ®ä¸­æå–é‡è¦çš„äº‹å®ä¿¡æ¯ã€‚

ç”¨æˆ·ID: {user_id}
æ•°æ®æ¥æº: {source_name}
åŸå§‹æ•°æ®:
{raw_data}

è¯·æå–å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
1. ç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯ï¼ˆå§“åã€å¹´é¾„ã€èŒä¸šç­‰ï¼‰
2. ç”¨æˆ·çš„åå¥½å’Œä¹ æƒ¯
3. ç”¨æˆ·çš„é‡è¦è¡Œä¸ºæˆ–äº‹ä»¶
4. å…¶ä»–å€¼å¾—è®°å¿†çš„ä¿¡æ¯

è§„åˆ™ï¼š
- åªæå–æ˜ç¡®çš„äº‹å®ä¿¡æ¯ï¼Œä¸è¦æ¨æµ‹
- æ¯æ¡ä¿¡æ¯åº”è¯¥æ˜¯ä¸€ä¸ªå®Œæ•´ã€ç‹¬ç«‹çš„é™ˆè¿°
- ä½¿ç”¨ç®€æ´ã€è‡ªç„¶çš„ä¸­æ–‡è¡¨è¾¾
- å¦‚æœæ²¡æœ‰æ˜ç¡®çš„äº‹å®ä¿¡æ¯ï¼Œè¿”å›ç©ºåˆ—è¡¨

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{"facts": ["äº‹å®1", "äº‹å®2", ...]}
```

### å¯é…ç½®çš„æç¤ºè¯å‚æ•°

åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ä»¥è‡ªå®šä¹‰æç¤ºè¯ï¼š

```json
{
  "summarization": {
    "enabled": true,
    "prompt_template": "ä½ çš„è‡ªå®šä¹‰æç¤ºè¯...",
    "max_tokens": 2000,
    "batch_size": 10,
    "extract_categories": ["åŸºæœ¬ä¿¡æ¯", "åå¥½", "è¡Œä¸º", "å…¶ä»–"]
  }
}
```

## ğŸš€ ä½¿ç”¨æ­¥éª¤

1. **åˆ›å»ºé…ç½®æ–‡ä»¶**: åœ¨ `api/app/` ç›®å½•ä¸‹åˆ›å»º `external_sync_config.json`
2. **é…ç½®æ•°æ®æº**: æ·»åŠ å¤–éƒ¨æ•°æ®åº“è¿æ¥ä¿¡æ¯å’ŒæŸ¥è¯¢è¯­å¥
3. **é…ç½®ç”¨æˆ·æ˜ å°„**: è®¾ç½®ç”¨æˆ·IDçš„æ˜ å°„è§„åˆ™
4. **å¯åŠ¨æœåŠ¡**: ç³»ç»Ÿä¼šè‡ªåŠ¨æŒ‰é…ç½®é—´éš”åŒæ­¥æ•°æ®
5. **æ‰‹åŠ¨è§¦å‘**: é€šè¿‡APIæ¥å£ `POST /api/v1/external-sync/trigger` æ‰‹åŠ¨è§¦å‘åŒæ­¥

## âš ï¸ æ³¨æ„äº‹é¡¹

1. ç¡®ä¿å¤–éƒ¨æ•°æ®åº“çš„è¿æ¥ä¿¡æ¯æ­£ç¡®
2. ç”¨æˆ·IDæ˜ å°„è§„åˆ™è¦ä¸æœ¬åœ°ç³»ç»ŸåŒ¹é…
3. å»ºè®®å…ˆåœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯é…ç½®
4. ç›‘æ§åŒæ­¥æ—¥å¿—ï¼ŒåŠæ—¶å‘ç°é—®é¢˜
5. åˆç†è®¾ç½®åŒæ­¥é—´éš”ï¼Œé¿å…å¯¹å¤–éƒ¨æ•°æ®åº“é€ æˆå‹åŠ›
